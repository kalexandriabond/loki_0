{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description**\n",
    "<br>\n",
    "Performing successfully in a volatile environment requires making fast, accurate decisions and updating those decisions given environmental feedback. However, accumulator models of choice-making, which model mechanisms internal to the decision, and reinforcement learning models, which involve how the outcome of those choices influence decision updates, are often isolated, despite their complementary roles in forging adaptive behavior. To more fully understand decision making and learning in a dynamic environment, I plan to explore how value conflict between competing actions (the degree to which the value associated with each action is similar) and the volatility of feedback (the change point frequency of mean value-action associations) influence adaptive decision making using a combined reinforcement learning and drift diffusion model. Specifically, quasi-Bayesian estimates of the value difference between targets [$B$] and change point probability [$\\Omega$] will serve as learning signals to update decision making parameters under conditions of varying volatility and conflict. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypotheses**\n",
    "<br>\n",
    "*Mechanism*\n",
    "<br>\n",
    "Either the rate of evidence accumulation [drift rate, $v$] or the starting point for evidence accumulation [$z$] will vary with conflict, such that larger differences in value either increase the drift rate or bias the starting point toward the higher-value target, and smaller differences in value decrease the drift rate or decrease starting point bias (so that $z$ is closer to $a$/2).\n",
    "<br>\n",
    "$$v_{t+1} = \\hat\\beta*B_{t} + v_{t}$$\n",
    "$$z_{t+1} = \\hat\\beta*B_{t} + z_{0}$$\n",
    "<br>\n",
    "The decision threshold [$a$] will increase as volatility increases and decrease as volatility decreases. Increased volatility will increase learning rates [$\\beta$]. \n",
    "\n",
    "$$a_{t+1} = \\hat\\beta*\\Omega_{t} + a_{0}$$\n",
    "\n",
    "*Behavior*\n",
    "<br>\n",
    "As a consequence of the above mechanisms, I predict that accuracy will decrease as volatility and conflict increase. Reaction times will increase more quickly under conditions of high volatility than high conflict, which will show a slow increase in reaction time as the learner disambiguates the value difference between targets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Simulations**\n",
    "<br>\n",
    "*Mechanism*\n",
    "<br>\n",
    "Either the rate of evidence accumulation [drift rate, $v$] or the starting point for evidence accumulation [$z$] will vary with conflict, such that larger differences in value either increase the drift rate or bias the starting point toward the higher-value target, and smaller differences in value decrease the drift rate or decrease starting point bias (so that $z$ is closer to $a$/2).\n",
    "<br>\n",
    "$$v_{t+1} = \\hat\\beta*B_{t} + v_{t}$$\n",
    "$$z_{t+1} = \\hat\\beta*B_{t} + z_{0}$$\n",
    "<br>\n",
    "The decision threshold [$a$] will increase as volatility increases and decrease as volatility decreases. Increased volatility will increase learning rates [$\\beta$]. \n",
    "\n",
    "$$a_{t+1} = \\hat\\beta*\\Omega_{t} + a_{0}$$\n",
    "\n",
    "*Behavior*\n",
    "<br>\n",
    "As a consequence of the above mechanisms, I predict that accuracy will decrease as volatility and conflict increase. Reaction times will increase more quickly under conditions of high volatility than high conflict, which will show a slow increase in reaction time as the learner disambiguates the value difference between targets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables**\n",
    "<br>\n",
    "*Predictor*<br> \n",
    "> *  conflict (high/low, qualitative)<br>\n",
    "*  volatility (high/low, qualitative)<br>\n",
    "\n",
    "*Response*<br>\n",
    ">*behavioral*\n",
    ">> *  accuracy (qualitative, 0/1)<br>\n",
    "*  reaction time (quantitative)<br>\n",
    "\n",
    "> *parameters from model fits to behavioral data*<br> \n",
    ">> *  decision boundary height [$a$] (quantitative)<br>\n",
    "*  drift rate [$v$] (quantitative)<br>\n",
    "*  starting point [$z$] (quantitative)<br>\n",
    "*  learning rate [$\\beta$] (quantitative)<br>\n",
    "\n",
    "> *learning signals from ideal observer*<br> \n",
    ">> *  change point probability [$\\Omega$] (quantitative)<br>\n",
    "*  belief in the reward difference between targets [$B$] (quantitative)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of observations**:\n",
    "6 participants with four 1000-trial sessions each. \n",
    "<br>\n",
    "**Access**:\n",
    "I estimate that I'll have the above data set by the end of February. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
